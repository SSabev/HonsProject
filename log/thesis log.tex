\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Thesis log}
\author{Stefan Sabev}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Week 3}

In the model have:
\begin{itemize}
\item last 4 friday weights
\item the twitter counts
\end{itemize}

Those would be the things that change.

\begin{enumerate}
\item Implement the method in the Ryan Adams \& David MacKay paper.
\item Run this on some toy data.
\item Run this on the search volumes without the Twitter data.
\item Histogram of the change point to the nearest twitter peak.
\end{enumerate}

Non-linear function of the features.

And SV regression with polynomial kernel.

Use relative error or use absolute error.

Average across days and destinations. 

Do a histogram of median, box plot or any ways to drill down and show it clearly.
\begin{enumerate}
\item take quantiles across destination. 
\item Median/mean absolute error.
\end{enumerate}

Take mean over days or median over destinations.
Cluster the destinations by popularity - split into 5 groups.
report the mean absolute error for each of the groups.

(Add something to the prior that makes change point more likely if there is a twitter spike)

\section{Week 5}

What's been tried so far:

\begin{itemize}
\item I have used smoothing to smooth the weekly seasonality component out of both the twitter and searches data.
That has yielded small improvement in the correlation coefficients.
\item I have also used a very basic method of prediction which works as follows:
\begin{quotation}
Calculate the mean and the standard deviation of the searches.
If the standard deviation is more than the mean, then there has been a spike which has pushed it higher.
\end{quotation}
\item I've also used that to determine which destinations should have a classifier built. That has yielded very small improvements.
\end{itemize}


The fact that the simple combination of LASSO + Ridge regression does not perform miraculously well has led my supervisor and I to believe that perhaps we should investigate more sophisticated models that will perhaps model the problem better.

I am currently doing:

\begin{itemize}
\item Reading the Adams \& MacKay paper on Bayesian change point models
\item Will look into the matlab implementation and try to port it to Python.
\end{itemize}


\textbf{Notes from today:}

In the model have:

- last 4 friday weights

- the twitter counts

Those would be the things that change.

\begin{enumerate}
\item Implement the method in the Ryan Adams \& David MacKay paper.
\item Run this on some toy data.
\item Run this on the search volumes without the Twitter data.
\item Histogram of the change point to the nearest twitter peak.
\end{enumerate}

Non-linear function of the features.

And SV regression with polynomial kernel.

Use relative error or use absolute error.

Average across days and destinations. 

Do a histogram of median, box plot or any ways to drill down and show it clearly.
\begin{enumerate}
\item take quantiles across destination. 
\item Median/mean absolute error.
\end{enumerate}

Take mean over days or median over destinations.

Cluster the destinations by popularity - split into 5 groups.

report the mean absolute error for each of the groups.

(Add something to the prior that makes change point more likely if there is a twitter spike)

1/T ( sum (y prediction - y actual) / y actual )


Having read the Adams and MacKay paper the well-log data seems to be the most relevant, because of the step changes observed.

After reading the paper I started the experimental part. 

Both of online and offline work perfectly fine.

Currently doing some other assignments, but will make a start on this soon.

\section{Week 6}

The algorithm does not work out of the box which is expected. 

What should be used for evaluation: 1/T ( sum (y prediction - y actual) / y actual )

Things to try:

\begin{enumerate}
\item AR(1)
\item and then AR(K)
\end{enumerate}

do it on weekly average or other data.



according to the formula - do the log likelihood before and after the proposed change point - read kevin murphy.

train two models at the 


for the presentation: give sochi and ukraine as examples.





\end{document}  